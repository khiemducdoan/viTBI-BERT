{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_loader \n",
    "import sklearn \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data1: torch.Size([4, 768])\n",
      "Shape of data2: torch.Size([4, 768])\n",
      "logits1 shape: torch.Size([4, 4])\n",
      "logits1 values:\n",
      " tensor([[ 0.3866, -0.3396,  0.2579,  0.2788],\n",
      "        [-0.1393, -0.0660,  0.9027, -0.6021],\n",
      "        [-0.4014,  0.8838,  0.1307,  1.0273],\n",
      "        [-0.0736,  0.1333, -1.3827, -0.0546]], grad_fn=<AddmmBackward0>)\n",
      "logits2 shape: torch.Size([4, 4])\n",
      "logits2 values:\n",
      " tensor([[ 0.5090,  0.1629, -0.6491, -0.2242],\n",
      "        [-0.6609,  0.4547,  0.4353, -0.0064],\n",
      "        [-0.1772, -1.3830,  0.3625,  0.6326],\n",
      "        [-0.9981,  0.8459, -0.3374,  0.4102]], grad_fn=<AddmmBackward0>)\n",
      "combined_logits shape: torch.Size([4, 4])\n",
      "combined_logits values:\n",
      " tensor([[ 0.8956, -0.1767, -0.3912,  0.0546],\n",
      "        [-0.8002,  0.3887,  1.3380, -0.6085],\n",
      "        [-0.5786, -0.4992,  0.4933,  1.6599],\n",
      "        [-1.0717,  0.9792, -1.7201,  0.3555]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_synthetic_data(batch_size=4, input_dim=768):\n",
    "    # Create random tensors to simulate input data\n",
    "    data1 = torch.randn(batch_size, input_dim)\n",
    "    data2 = torch.randn(batch_size, input_dim)\n",
    "    return data1, data2\n",
    "\n",
    "# Define a simple linear layer for processing the data\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Main function to process synthetic data\n",
    "def main():\n",
    "    batch_size = 4\n",
    "    input_dim = 768\n",
    "    output_dim = 4\n",
    "\n",
    "    # Generate synthetic data\n",
    "    data1, data2 = generate_synthetic_data(batch_size, input_dim)\n",
    "\n",
    "    # Print the shapes of the data tensors\n",
    "    print(\"Shape of data1:\", data1.shape)\n",
    "    print(\"Shape of data2:\", data2.shape)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = SimpleLinearModel(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "    # Process the data through the linear layer\n",
    "    logits1 = model(data1)\n",
    "    logits2 = model(data2)\n",
    "\n",
    "    # Add the two logits\n",
    "    combined_logits = logits1 + logits2\n",
    "\n",
    "    # Print the shapes and values for verification\n",
    "    print(\"logits1 shape:\", logits1.shape)\n",
    "    print(\"logits1 values:\\n\", logits1)\n",
    "    print(\"logits2 shape:\", logits2.shape)\n",
    "    print(\"logits2 values:\\n\", logits2)\n",
    "    print(\"combined_logits shape:\", combined_logits.shape)\n",
    "    print(\"combined_logits values:\\n\", combined_logits)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.ViTBERT(data_path=\"/media/data3/home/khiemdd/ViTBERT/dataset/data500/donedataset_after.csv\",\n",
    "                                      stop_words_file= \"/media/data3/home/khiemdd/ViTBERT/dataset/needed_files/vietnamese-stopwords.txt\",\n",
    "                                      wordnet_file= \"/media/data3/home/khiemdd/ViTBERT/dataset/needed_files/word_net_vi.json\",\n",
    "                                      indices= None,\n",
    "                                      type = \"test\",\n",
    "                                      tokenizer=\"demdecuong/vihealthbert-base-word\" ) # Ensure ViTBERTDataset is implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n",
      "  \n",
      "train ids: \n",
      "<class 'numpy.ndarray'>\n",
      "test ids: \n",
      "<class 'numpy.ndarray'>\n",
      "fold 1:\n",
      "  \n",
      "train ids: \n",
      "<class 'numpy.ndarray'>\n",
      "test ids: \n",
      "<class 'numpy.ndarray'>\n",
      "fold 2:\n",
      "  \n",
      "train ids: \n",
      "<class 'numpy.ndarray'>\n",
      "test ids: \n",
      "<class 'numpy.ndarray'>\n",
      "fold 3:\n",
      "  \n",
      "train ids: \n",
      "<class 'numpy.ndarray'>\n",
      "test ids: \n",
      "<class 'numpy.ndarray'>\n",
      "fold 4:\n",
      "  \n",
      "train ids: \n",
      "<class 'numpy.ndarray'>\n",
      "test ids: \n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"fold {fold}:\\n  \")\n",
    "    print(\"train ids: \")\n",
    "    print(type(train_ids))\n",
    "    print(\"test ids: \")\n",
    "    print(type(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 181\u001b[0m\n\u001b[1;32m    178\u001b[0m augmented_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_TRAINING_AUGMENT.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Plot distributions\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mplot_label_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLabel Distribution Before Augmentation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m plot_label_distribution(augmented_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel Distribution After Augmentation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 169\u001b[0m, in \u001b[0;36mplot_label_distribution\u001b[0;34m(df, title)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_label_distribution\u001b[39m(df, title):\n\u001b[0;32m--> 169\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m    170\u001b[0m     sns\u001b[38;5;241m.\u001b[39mcountplot(data\u001b[38;5;241m=\u001b[39mdf, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(title)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from random import shuffle\n",
    "from mtranslate import translate\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "class DataAugmentation:\n",
    "    def __init__(self, stop_words_file, wordnet_file, seed=1):\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        \n",
    "        self.stop_words = self.load_stop_words(stop_words_file)\n",
    "        self.wordnet_data = self.load_wordnet(wordnet_file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_stop_words(file_path):\n",
    "        stop_words = []\n",
    "        with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                stop_words.append(line.strip())\n",
    "        return stop_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_wordnet(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"WordNet file not found.\")\n",
    "            return {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def back_translation(sentence, intermediate_langs=['en', 'fr', 'ru']):\n",
    "        intermediate_lang = random.choice(intermediate_langs)\n",
    "        translated_sentence = translate(sentence, intermediate_lang)\n",
    "        back_translated_sentence = translate(translated_sentence, 'vi')\n",
    "        return back_translated_sentence\n",
    "    \n",
    "    def get_synonyms(self, word):\n",
    "        synonyms = set()\n",
    "        for key, value in self.wordnet_data.items():\n",
    "            if key.strip() == word:\n",
    "                synonyms.update([v.strip() for v in value])\n",
    "        synonyms.discard(word)  # Remove the word itself if present\n",
    "        return list(synonyms)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_deletion(words, p):\n",
    "        if len(words) == 1:\n",
    "            return words\n",
    "        new_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "        return new_words if new_words else [random.choice(words)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_swap(words, n):\n",
    "        for _ in range(n):\n",
    "            if len(words) > 1:\n",
    "                idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "                words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        return words\n",
    "    \n",
    "    def random_insertion(self, words, n):\n",
    "        for _ in range(n):\n",
    "            synonyms = []\n",
    "            while not synonyms:\n",
    "                random_word = random.choice(words)\n",
    "                synonyms = self.get_synonyms(random_word)\n",
    "            random_synonym = random.choice(synonyms)\n",
    "            random_idx = random.randint(0, len(words))\n",
    "            words.insert(random_idx, random_synonym)\n",
    "        return words\n",
    "    \n",
    "    def synonym_replacement(self, words, n):\n",
    "        new_words = words.copy()\n",
    "        random_word_list = [word for word in words if word not in self.stop_words]\n",
    "        shuffle(random_word_list)\n",
    "        num_replaced = 0\n",
    "        for random_word in random_word_list:\n",
    "            synonyms = self.get_synonyms(random_word)\n",
    "            if synonyms:\n",
    "                synonym = random.choice(synonyms)\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "                if num_replaced >= n:\n",
    "                    break\n",
    "        return new_words\n",
    "    \n",
    "    def eda(self, sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9, bt_langues=['en', 'fr', 'ru']):\n",
    "        words = sentence.split(' ')\n",
    "        num_words = len(words)\n",
    "        augmented_sentences = [sentence]\n",
    "    \n",
    "        if num_words > 1:\n",
    "            num_new_per_technique = int(num_aug / 5) + 1  # Updated to account for back_translation too\n",
    "            n_sr = max(1, int(alpha_sr * num_words))\n",
    "            n_ri = max(1, int(alpha_ri * num_words))\n",
    "            n_rs = max(1, int(alpha_rs * num_words))\n",
    "    \n",
    "            for _ in range(num_new_per_technique):\n",
    "                augmented_sentences.append(' '.join(self.synonym_replacement(words, n_sr)))\n",
    "            for _ in range(num_new_per_technique):\n",
    "                augmented_sentences.append(' '.join(self.random_insertion(words, n_ri)))\n",
    "            for _ in range(num_new_per_technique):\n",
    "                augmented_sentences.append(' '.join(self.random_swap(words, n_rs)))\n",
    "            for _ in range(num_new_per_technique):\n",
    "                augmented_sentences.append(' '.join(self.random_deletion(words, p_rd)))\n",
    "            for _ in range(num_new_per_technique):\n",
    "                augmented_sentences.append(self.back_translation(sentence, intermediate_langs=bt_langues))\n",
    "        \n",
    "        return list(set(augmented_sentences))\n",
    "    \n",
    "    def edafor3(self, sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9, bt_langues=['en', 'fr', 'ru']):\n",
    "        words = sentence.split(' ')\n",
    "        num_words = len(words)\n",
    "        augmented_sentences = [sentence]\n",
    "    \n",
    "        if num_words > 1:\n",
    "            num_new_per_technique = int(num_aug / 5) + 1  # Updated to account for back_translation too\n",
    "            n_sr = max(1, int(alpha_sr * num_words))\n",
    "            n_ri = max(1, int(alpha_ri * num_words))\n",
    "            n_rs = max(1, int(alpha_rs * num_words))\n",
    "    \n",
    "            augmented_sentences.append(self.back_translation(sentence, intermediate_langs=bt_langues))\n",
    "        \n",
    "        return list(set(augmented_sentences))\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_punctuation(sentence):\n",
    "        return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    \n",
    "    def augment_dataframe(self, df, num_aug=9, alpha=0.1, max_aug_for_3=40):\n",
    "        augmented_rows = []\n",
    "        augment_count_3 = 0\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            original_col1 = row[df.columns[0]].strip()\n",
    "            label = str(row[df.columns[1]]).strip()\n",
    "    \n",
    "            if label == \"1.0\" or label == \"4.0\":\n",
    "                augmented_col1 = self.eda(original_col1, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)\n",
    "                for sent1 in augmented_col1:\n",
    "                    augmented_rows.append({df.columns[0]: self.clear_punctuation(sent1), df.columns[1]: float(label)})\n",
    "            elif label == \"3.0\" and augment_count_3 < max_aug_for_3:\n",
    "                augmented_col1 = self.edafor3(original_col1, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=1)\n",
    "                for sent1 in augmented_col1:\n",
    "                    augmented_rows.append({df.columns[0]: self.clear_punctuation(sent1), df.columns[1]: float(label)})\n",
    "                augment_count_3 += 1\n",
    "            else:\n",
    "                augmented_rows.append(row.to_dict())\n",
    "    \n",
    "        return pd.DataFrame(augmented_rows)\n",
    "\n",
    "# Example Usage\n",
    "stop_words_file = r\"/media/data3/home/khiemdd/ViTBERT/dataset/needed_files/vietnamese-stopwords.txt\"\n",
    "wordnet_file = r\"/media/data3/home/khiemdd/ViTBERT/dataset/needed_files/word_net_vi.json\"\n",
    "\n",
    "data_augmentor = DataAugmentation(stop_words_file, wordnet_file)\n",
    "\n",
    "df = pd.read_csv('/media/data3/home/khiemdd/ViTBERT/dataset/datakfold/after/fold1_train.csv')\n",
    "augmented_df = data_augmentor.augment_dataframe(df, num_aug=2, alpha=0.1, max_aug_for_3= 40)\n",
    "augmented_df.to_csv('DATA_TRAINING_AUGMENT.csv', index=False)\n",
    "\n",
    "# Load the augmented dataset\n",
    "dataset_final_train = pd.read_csv(\"DATA_TRAINING_AUGMENT.csv\")\n",
    "# Function to plot label distribution\n",
    "def plot_label_distribution(df, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x='label')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "# Load the original and augmented datasets\n",
    "original_df = pd.read_csv('/media/data3/home/khiemdd/ViTBERT/dataset/datakfold/after/fold1_train.csv')\n",
    "augmented_df = pd.read_csv(\"DATA_TRAINING_AUGMENT.csv\")\n",
    "\n",
    "# Plot distributions\n",
    "plot_label_distribution(original_df, 'Label Distribution Before Augmentation')\n",
    "plot_label_distribution(augmented_df, 'Label Distribution After Augmentation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
